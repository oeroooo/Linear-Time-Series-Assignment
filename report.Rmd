---
title: Linear Time Series Assignment
subtitle: 
date: 2023-2024
author: 
 - Romain Delhommais
 - Jacques Zhang
editor: ENSAE
output: 
  pdf_document:
      latex_engine: xelatex
---

```{knitr setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r, echo=FALSE, include=FALSE}
library(dplyr)
library(kableExtra)
library(astsa)
library(forecast)
library(portes)
library(fUnitRoots)
library(tseries)
library(gridExtra)
library(grid)
# library(knitr)
library(ggplot2)
library(gtable)
```


# The data 
## The studied temporal series
```{r, echo=FALSE}
Time_series <- read.csv("data/valeurs_mensuelles.csv", sep = ";") %>%  
  filter(!row_number() %in% c(1, 2, 3)) 

names(Time_series) <- c("date", "value", "codes")
Time_series$date <- as.Date(paste(Time_series$date, "-01", sep=""))
Time_series$value <- as.numeric(Time_series$value)


caracteristiques <- read.csv("data/caractéristiques.csv", sep = ";")
```
In this project, we will study the monthly series of the fabrication of french weapons and ammo from 1990 to 2024. This series is accessible 
in the INSEE website https://www.insee.fr/fr/statistiques/serie/010767975.
The initial series is ploted on the following graph:

```{r, echo=FALSE}
plotly::plot_ly(x = Time_series$date, y = Time_series$value, type = "scatter", mode = "lines") %>%
  plotly::layout(title = "Monthly series of the fabrication of french weapons and ammo from 1990 to 2024",
                 xaxis = list(title = "Date"),
                 yaxis = list(title = "Value"))
```

            yaxis = list(title = "Value"))





We see that the series is not stationary, and it clearly has a trend. 

## Transformation of the series 
We will differentiate the series to remove the trend. 

```{r, echo=FALSE}
diff <- diff(Time_series$value)
```
We will plot the differentiated series to see if the trend has been removed.
```{r, echo=FALSE}
plotly::plot_ly(x = Time_series$date[-1], y = diff, type = "scatter", mode = "lines") %>%
  plotly::layout(title = "Log return of the series",
                 xaxis = list(title = "Date"),
                 yaxis = list(title = "Value"))
```

The log return of the series seems to be stationary. We can already check the ACF and PACF of the differentiated series to get an idea of the p and q parameters of the ARMA model.
```{r, echo=FALSE}
png("images/acf.png")
acf(diff)
dev.off()
png("images/pacf.png")
pacf(diff)
dev.off()
```



We will test the stationarity of the series using the Augmented Dickey-Fuller (ADF) test, the Phillips-Perron (PP) test and the KPSS stationnarity test. We remind that the null hypothesis of the ADF and PP tests is that the series is not stationary and the null hypothesis of the KPSS test is that the series is stationary.
```{r, echo=FALSE, include=FALSE}
adf <- adfTest(diff)
pp <- pp.test(diff)
kpss <- kpss.test(diff)
```
Here are the results of the tests:
```{r, echo=FALSE}
 data.frame(Test = c("Augmented Dickey-Fuller", "Phillips-Perron", "KPSS"), 
                 Statistics = c(adf@test$statistic, pp$statistic, kpss$statistic), 
                  lag_order = c(adf@test$parameter, pp$parameter, kpss$parameter), 
                  P_value = c(paste0("<= ", adf@test$p.value), paste0("<= ", pp$p.value), paste0(">= ", kpss$p.value)))

```
The ADF and PP tests gave us p-values inferior to 1%, hence we can reject the null hypothesis in the 1% level, and the KPSS test gave us a p-value superior to 10% we cannot reject the null hypothesis in the 10% level. Hence we can consider that the differentiated series is stationary.
## Representation of the series before and after transformation

```{r, echo=FALSE}
png("images/series.png")
plot.zoo(Time_series$date, Time_series$value, type = "l", col = "blue", xlab = "Date", ylab = "Value")
dev.off()
png("images/diff.png")
plot.zoo(Time_series$date[-1], diff, type = "l", col = "red", xlab = "Date", ylab = "Value")
dev.off()
```



# ARMA models
## Selection of the arma model
We see that the tests of the stationarity consider a maximum lag of 5. Hence we will select our p and q parameters in the ARMA model between 0 and 6. 
To get the best model, we will use the AIC and BIC criteria.
## AIC and BIC criteria
AIC and BIC for the differents arma(i,j) models where i and j are between 0 and 6.
```{r, echo=FALSE}
AIC_BIC <- data.frame(p = integer(), q = integer(), AIC = numeric(), BIC = numeric())
for (i in 0:6){
  for (j in 0:6){
    model <- arima(diff, order = c(i, 0, j))
    AIC_BIC <- rbind(AIC_BIC, data.frame(p = i, q = j, AIC = AIC(model), BIC = BIC(model)))
  }
}

kable(AIC_BIC) %>%
  kableExtra::kable_styling(bootstrap_options = "striped", full_width = F)
```
For a model (p, q), it's considered a significant model if the absolute value of the fraction of the estimated coefficient and its standard error is superior to 1.96 for the highest coefficient associated to autoregressif and moving average terms. 
In the table we represent this information with YES and NO as the first coordinate of our table. Furthermore, a model is considered as valid if the p value of the Ljung-Box test is superior to 0.05. We will represent this information with YES and NO as the second coordinate of our table.
```{r, echo=FALSE}
# Initialize the dataframe
validity <- data.frame(p = integer(), q = integer(), Valid_AR_MA = character(), Valid_Ljung_Box = character())

# Loop over potential orders of p and q
for (i in 0:6) {
  for (j in 0:6) {
    # Fit the ARIMA model
    model <- arima(diff, order = c(i, 0, j))
    coef_model <- coef(model)
    vcov_model <- sqrt(diag(vcov(model)))
    
    # Check the highest order AR and MA coefficients for significance
    AR_valid <- "NO"  # Default as NO
    MA_valid <- "NO"
    if ("ar1" %in% names(coef_model) && i > 0) {  # Checking the highest AR coefficient
      AR_valid <- ifelse(abs(coef_model[paste0("ar", i)] / vcov_model[paste0("ar", i)]) > 1.96, "YES", "NO")
    }
    if ("ma1" %in% names(coef_model) && j > 0) {  # Checking the highest MA coefficient
      MA_valid <- ifelse(abs(coef_model[paste0("ma", j)] / vcov_model[paste0("ma", j)]) > 1.96, "YES", "NO")
    }
    
    # Combine AR and MA validation results
    Valid_AR_MA <- if(AR_valid == "YES" && MA_valid == "YES") "YES" else "NO"
    
    # Ljung-Box test on residuals to check for autocorrelation at lag 5
    Ljung_Box_p_value <- Box.test(model$residuals, lag = max(5, i, j), type = "Ljung-Box")$p.value
    Valid_Ljung_Box <- ifelse(Ljung_Box_p_value > 0.05, "YES", "NO")
    
    # Append the results to the dataframe
    validity <- rbind(validity, data.frame(p = i, q = j, Valid_AR_MA = Valid_AR_MA, Valid_Ljung_Box = Valid_Ljung_Box))
  }
}
validity %>%
  kable() %>%
  kableExtra::kable_styling(bootstrap_options = "striped", full_width = F)
```
Hence the valid models are the following:
```{r, echo=FALSE}
validity %>%
  filter(Valid_AR_MA == "YES" & Valid_Ljung_Box == "YES") %>%
  kable() %>%
  kableExtra::kable_styling(bootstrap_options = "striped", full_width = F)

```
The model with the lowest AIC and BIC for the valid models is the following:
```{r, echo=FALSE}
validity %>%
  filter(Valid_AR_MA == "YES" & Valid_Ljung_Box == "YES") %>%
  left_join(AIC_BIC, by = c("p", "q")) %>%
  filter(AIC == min(AIC)) %>%
  kable() %>%
  kableExtra::kable_styling(bootstrap_options = "striped", full_width = F)

validity %>%
  filter(Valid_AR_MA == "YES" & Valid_Ljung_Box == "YES") %>%
  left_join(AIC_BIC, by = c("p", "q")) %>%
  filter(BIC == min(BIC)) %>%
  kable() %>%
  kableExtra::kable_styling(bootstrap_options = "striped", full_width = F)
```

Hence the model (4,5) is the best model according to the AIC criteria and the model (2, 3) is the best model according to BIC criteria. 
We will test the significance of the models (4, 5) and (2, 3) by a student test. We will have as null hypothesis that the coefficients of the model are equal to 0. 
```{r, echo=FALSE}
model <- arima(diff, order = c(4, 0, 5))
p_value <- (1 - pnorm(abs(coef(model)) / sqrt(diag(model$var.coef)))) * 2
p_value %>% as.data.frame() %>% setNames(c("p_value")) %>% kable() %>% kableExtra::kable_styling(bootstrap_options = "striped", full_width = F)
```
The model is well adjusted as the coefficients are significant at the 5% level.
```{r, echo=FALSE}
model <- arima(diff, order = c(2, 0, 3))
p_value <- (1 - pnorm(abs(coef(model)) / sqrt(diag(model$var.coef)))) * 2
p_value %>% as.data.frame() %>% setNames(c("p_value")) %>% kable() %>% kableExtra::kable_styling(bootstrap_options = "striped", full_width = F)
```
The model is not well adjusted as the coefficients of MA(2) are not significant at the 5% level. 
Hence we will choose the model (4, 5) as the best model.
## Normality of residuals
In order to obtain the most reliable prediction, it's good to have normal residuals. We can check using QQ plot if the residuals are normally distributed, ie if the residuals are aligned with the normal distribution. 


```{r, echo=FALSE}
model <- arima(diff, order = c(4, 0, 5))
residuals <- model$residuals
png("images/qqplot.png")
qqnorm(residuals)
qqline(residuals)
dev.off()
```

We can see that our residuals are not normally distributed mostly in the extremes. We can confirm that using the Jarque-Bera test which is a test of the residuals for normality. The null hypothesis is that the residuals are normally distributed. 
```{r, echo=FALSE}
jarque.bera.test(residuals)
```
The p-value is less than 0.05, hence we reject the null hypothesis and conclude that the residuals are not normally distributed.

Finally, for our differentiated series we selected the model (4, 5) with non normal residuals. Here are our estimated coefficients:
```{r, echo=FALSE}
arima(diff, order = c(4, 0, 5))
```
Xt = 0.06 - 0.96 Xt-1 - 0.22 Xt-2 - 0.91 Xt-3 - 0.88 Xt-4 + 0.47 Et-1 - 0.26 Et-2 + 0.86 Et-3 + 0.46 Et-4 - 0.53 Et-5

# Prediction
## Confidence region of level α
## Hypothesis
## Graphic representation
## Open question